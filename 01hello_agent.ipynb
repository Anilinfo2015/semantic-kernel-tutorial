{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Semantic Kernel\n",
    "\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets you easily build AI agents and integrate the latest AI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install semantic-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create .env file and add following properties\n",
    "\n",
    "GLOBAL_LLM_SERVICE=\"AzureOpenAI\"\n",
    "AZURE_OPENAI_API_KEY=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\"\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"gpt-4o-mini\"\n",
    "AZURE_OPENAI_TEXT_DEPLOYMENT_NAME=\"gpt-4o-mini\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=\"...\"\n",
    "AZURE_OPENAI_API_VERSION=\"2023-03-15-preview\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109351/3501489966.py:26: DeprecationWarning: Use the `FunctionChoiceBehavior` `Auto` class method instead.\n",
      "  execution_settings.function_call_behavior = FunctionCallBehavior.EnableFunctions(\n",
      "The `function_call_behavior` parameter is deprecated. Please use the `function_choice_behavior` parameter instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > I'm just a program, but I'm here and ready to help you! How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109351/3501489966.py:67: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  await main()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_call_behavior import FunctionCallBehavior\n",
    "\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Initialize the kernel\n",
    "    kernel = Kernel()\n",
    "    # Add Azure OpenAI chat completion\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=\"default\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Enable planning\n",
    "    execution_settings = AzureChatPromptExecutionSettings(tool_choice=\"auto\")\n",
    "    execution_settings.function_call_behavior = FunctionCallBehavior.EnableFunctions(\n",
    "        auto_invoke=True, filters={}\n",
    "    )\n",
    "\n",
    "    # Create a history of the conversation\n",
    "    history = ChatHistory()\n",
    "    # Collect user input\n",
    "    userInput = \"Hello, how are you?\"\n",
    "    history.add_user_message(userInput)\n",
    "\n",
    "    chat_function = kernel.add_function(\n",
    "        prompt=\"\"\"{{$chat_history}}{{$user_input}}\"\"\",\n",
    "        function_name=\"chat\",\n",
    "        plugin_name=\"chat\",\n",
    "        prompt_execution_settings=execution_settings,\n",
    "    )\n",
    "\n",
    "    # Get the response from the AI\n",
    "    result = kernel.invoke_stream(\n",
    "        chat_function,\n",
    "        user_input=userInput,\n",
    "        chat_history=history,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + await createSyncResponse(result))\n",
    "\n",
    "\n",
    "async def createSyncResponse(answer) -> str:\n",
    "    response = \"\"\n",
    "    async for message in answer:\n",
    "        response += str(message[0])\n",
    "    return response\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except RuntimeError:\n",
    "        # If already in an event loop, use await\n",
    "        await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-streaming chat completion\n",
    "To use non-streaming chat completion, you can use the following code to generate a response from the AI agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109351/1012246501.py:25: DeprecationWarning: Use the `FunctionChoiceBehavior` `Auto` class method instead.\n",
      "  execution_settings.function_call_behavior = FunctionCallBehavior.EnableFunctions(\n",
      "The `function_call_behavior` parameter is deprecated. Please use the `function_choice_behavior` parameter instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > Hello! I'm here and ready to help. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109351/1012246501.py:48: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  await main()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_call_behavior import FunctionCallBehavior\n",
    "\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Initialize the kernel\n",
    "    kernel = Kernel()\n",
    "    # Add Azure OpenAI chat completion\n",
    "    chat_completion = AzureChatCompletion(\n",
    "        service_id=\"default\",\n",
    "    )\n",
    "    kernel.add_service(chat_completion)\n",
    "\n",
    "    # Enable planning\n",
    "    execution_settings = AzureChatPromptExecutionSettings()\n",
    "    execution_settings.function_call_behavior = FunctionCallBehavior.EnableFunctions(\n",
    "        auto_invoke=True, filters={}\n",
    "    )\n",
    "\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(\"Hello, how are you?\")\n",
    "\n",
    "    response = await chat_completion.get_chat_message_content(\n",
    "        chat_history=chat_history,\n",
    "        kernel=kernel,\n",
    "        settings=execution_settings,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + response.content)\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except RuntimeError:\n",
    "        # If already in an event loop, use await\n",
    "        await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
